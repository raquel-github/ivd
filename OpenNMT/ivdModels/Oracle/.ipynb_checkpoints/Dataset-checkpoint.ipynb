{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import h5py\n",
    "# import\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_file = '../../../../ivd_data/Oracle/oracle.val.json'\n",
    "train_file = '../../../../ivd_data/Oracle/oracle.train.json'\n",
    "small_file = '../../../seq2seq/Preprocessing/Data/oracle.small.test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Yes',\n",
       " 'crop_features': '2416.jpg',\n",
       " 'game_id': 2416,\n",
       " 'img_features': 'COCO_val2014_000000534127.jpg',\n",
       " 'obj_cat': 1,\n",
       " 'question': 'Is it a person?',\n",
       " 'spatial': [0.0065, -0.1691, 0.3181, 0.7969, 0.1623, 0.3139, 0.1558, 0.483]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(train_file) as f:\n",
    "    data = json.load(f)['questions']\n",
    "print(len(data))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579633\n",
      "568524\n",
      "55219\n",
      "['let', 'us', 'start', 'counting', 'the', 'pigeons', 'standing', 'on', 'the', 'iron', 'bars', 'starting', 'with', 'the', 'one', 'from', 'the', 'left', 'that', 'we', 'can', 'see', 'his', 'shadow', 'we', 'have', 'one', 'with', 'a', 'shadow', '2', 'with', 'a', 'shadow', '3', 'first', 'with', 'no', 'shadow', 'is', 'it', 'one', 'of', 'them', '?']\n",
      "45\n",
      "['?']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "questions = [re.findall(r'\\w+', datum['question'].lower())+['?'] for datum in data]\n",
    "# questions = [re.findall(\"[a-zA-Z]+\", datum['question'].lower()) for datum in data]\n",
    "# questions = [datum['question'].lower().split() for datum in data]\n",
    "# for q in questions:\n",
    "#     for idx in range(len(q)):\n",
    "#         if q[idx][-1] == '?':\n",
    "#             q[idx] = q[idx][:-1]\n",
    "#         if idx == len(q)-1:\n",
    "#             q += '?'\n",
    "print(len(questions))\n",
    "max_length = max(questions,key=len)\n",
    "min_length = min(questions,key=len)\n",
    "print(questions.index(max_length))\n",
    "print(questions.index(min_length))\n",
    "print(max_length)\n",
    "print(len(max_length))\n",
    "print(min_length)\n",
    "print(len(min_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9920\n",
      "3700\n"
     ]
    }
   ],
   "source": [
    "vocab = defaultdict(int)\n",
    "vocab['-PAD-'] = 6\n",
    "vocab['-UNK-'] = 6\n",
    "\n",
    "for line in questions:\n",
    "    for word in line:\n",
    "        if word in vocab:\n",
    "            vocab[word] += 1\n",
    "        else:\n",
    "            vocab[word] = 1\n",
    "# vocab = list(set(vocab))\n",
    "print(len(vocab))\n",
    "vocab_temp = dict(vocab)\n",
    "for w in vocab_temp:\n",
    "    if vocab[w] < 5:\n",
    "        vocab.pop(w)\n",
    "del vocab_temp\n",
    "print(len(vocab))\n",
    "# for w in sorted(vocab, key=vocab.get, reverse=True):\n",
    "#       print(w, vocab[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2ind = {}\n",
    "ind2word = {}\n",
    "\n",
    "for idx, w in enumerate(sorted(vocab, key=vocab.get, reverse=True)):\n",
    "    ind2word[idx] = w\n",
    "    word2ind[w] = idx\n",
    "\n",
    "# print(ind2word)\n",
    "# print(word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# json_data = {'word2ind':word2ind, 'ind2word':ind2word}\n",
    "\n",
    "# with open('vocabOracle.json','w') as vc:\n",
    "#     json.dump(json_data, vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'No',\n",
       " 'crop_features': '2426.jpg',\n",
       " 'game_id': 2426,\n",
       " 'img_features': 'COCO_train2014_000000460809.jpg',\n",
       " 'obj_cat': 44,\n",
       " 'question': 'does the container have a picture?',\n",
       " 'spatial': [-0.3059,\n",
       "  -0.9893,\n",
       "  -0.2675,\n",
       "  -0.9128,\n",
       "  -0.2867,\n",
       "  -0.9511,\n",
       "  0.0192,\n",
       "  0.0382]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0393373   0.49363673 -0.23234019 ..., -0.1444326   0.16763082\n",
      "  0.14814836]\n",
      "46794\n"
     ]
    }
   ],
   "source": [
    "filename = '../../../../ivd_data/img_features/image_features.h5'\n",
    "\n",
    "h5data = h5py.File(filename, 'r')\n",
    "train_data = h5data['train_img_features']\n",
    "del h5data\n",
    "print(train_data[0])\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OracleDataset(Dataset):\n",
    "    def __init__(self, split, json_data_file, img_features_file, img2id_file, crop_features_file, crop2id_file, vocab_json_file):\n",
    "        \"\"\"\n",
    "        split: ['train', 'val', 'test']\n",
    "        \"\"\"\n",
    "        with open(json_data_file) as file:\n",
    "            self.questions = json.load(file)['questions']\n",
    "        with open(img2id_file) as file:\n",
    "            self.img2id = json.load(file)[split+'2id']\n",
    "        with open(crop2id_file) as file:\n",
    "            self.crop2id = json.load(file)[split+'crops2id']\n",
    "        \n",
    "        img_h5data = h5py.File(img_features_file, 'r')\n",
    "        self.img_features = img_h5data[split+'_img_features']\n",
    "        del img_h5data\n",
    "        crop_h5data = h5py.File(crop_features_file, 'r')\n",
    "        self.crop_features = crop_h5data[split+'_crop_features']\n",
    "        del crop_h5data\n",
    "        \n",
    "        self.ans2id = {'no':0, 'yes':1, 'n/a':2}\n",
    "        with open(vocab_json_file) as file:\n",
    "            self.word2ind = json.load(file)['word2ind']\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        crop_features = self.crop_features[self.crop2id[self.questions[idx]['crop_features']]]\n",
    "        img_features = self.img_features[self.img2id[self.questions[idx]['img_features']]]\n",
    "        spaital = torch.FloatTensor(self.questions[idx]['spatial'])\n",
    "        obj_cat = self.questions[idx]['obj_cat']\n",
    "        \n",
    "        raw_question = self.questions[idx]['question']\n",
    "        question = (np.ones(45,'uint8')*self.word2ind['-PAD-']).tolist()\n",
    "        for wid, word in enumerate(re.findall(r'\\w+', raw_question.lower())+['?']):\n",
    "            if word in self.word2ind:\n",
    "                question[wid] = self.word2ind[word]\n",
    "            else:\n",
    "                question[wid] = self.word2ind['-UNK-']\n",
    "            \n",
    "        question = torch.LongTensor(question)\n",
    "        answer = self.ans2id[self.questions[idx]['answer'].lower()]\n",
    "        \n",
    "        sample = {'question':question, 'answer': answer, 'crop_features':crop_features, 'img_features':img_features,\\\n",
    "                  'spaital':spaital, 'obj_cat':obj_cat}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 1,\n",
       " 'crop_features': array([ 0.05228886, -0.0009644 , -0.12459859, ..., -0.00792849,\n",
       "         0.19189964,  0.01208303], dtype=float32),\n",
       " 'img_features': array([ 0.15624177,  0.15925665, -0.10525729, ..., -0.07205705,\n",
       "         0.02671698, -0.07078305], dtype=float32),\n",
       " 'obj_cat': 23,\n",
       " 'question': \n",
       "     1\n",
       "     2\n",
       "     3\n",
       "   267\n",
       "     9\n",
       "     0\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       "  3252\n",
       " [torch.LongTensor of size 45],\n",
       " 'spaital': \n",
       " -0.3352\n",
       " -0.9196\n",
       "  0.6111\n",
       " -0.5880\n",
       "  0.1380\n",
       " -0.7538\n",
       "  0.4732\n",
       "  0.1658\n",
       " [torch.FloatTensor of size 8]}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'val'\n",
    "json_data_file = '../../../../ivd_data/Oracle/oracle.val.json'\n",
    "vocab_json_file = 'vocabOracle.json'\n",
    "img_features_file = '../../../../ivd_data/img_features/image_features.h5'\n",
    "img2id_file = '../../../../ivd_data/img_features/img_features2id.json'\n",
    "crop_features_file = '../../../../ivd_data/img_features/crop_features.h5'\n",
    "crop2id_file = '../../../../ivd_data/img_features/crop_features2id.json'\n",
    "\n",
    "od = OracleDataset(split, json_data_file, img_features_file, img2id_file, crop_features_file, crop2id_file, vocab_json_file)\n",
    "od[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16692805290222168\n",
      "{'question': \n",
      "    1     2    29  ...   3252  3252  3252\n",
      "    1     2     3  ...   3252  3252  3252\n",
      "    1     2     7  ...   3252  3252  3252\n",
      "       ...          ⋱          ...       \n",
      "    1     2     4  ...   3252  3252  3252\n",
      "    1     2    33  ...   3252  3252  3252\n",
      "    1     2     3  ...   3252  3252  3252\n",
      "[torch.LongTensor of size 128x45]\n",
      ", 'obj_cat': \n",
      " 23\n",
      " 23\n",
      "  8\n",
      "  8\n",
      "  8\n",
      "  8\n",
      " 43\n",
      " 43\n",
      " 43\n",
      " 63\n",
      " 63\n",
      " 63\n",
      " 63\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      " 59\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      " 51\n",
      " 51\n",
      " 51\n",
      " 73\n",
      " 73\n",
      " 73\n",
      " 73\n",
      " 73\n",
      " 73\n",
      "  3\n",
      "  3\n",
      "  3\n",
      "  3\n",
      "  3\n",
      "  3\n",
      "  1\n",
      "  1\n",
      "  2\n",
      "  2\n",
      "  2\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      " 13\n",
      " 13\n",
      " 20\n",
      " 20\n",
      " 20\n",
      " 55\n",
      " 55\n",
      " 55\n",
      " 55\n",
      " 55\n",
      " 55\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      " 17\n",
      " 17\n",
      " 20\n",
      " 20\n",
      " 20\n",
      " 20\n",
      " 20\n",
      " 20\n",
      " 20\n",
      " 20\n",
      " 38\n",
      " 38\n",
      "  1\n",
      "  1\n",
      " 75\n",
      " 75\n",
      " 75\n",
      " 75\n",
      " 75\n",
      " 75\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      " 90\n",
      " 90\n",
      " 82\n",
      " 82\n",
      " 82\n",
      " 82\n",
      " 82\n",
      " 24\n",
      " 24\n",
      " 46\n",
      " 46\n",
      " 46\n",
      " 46\n",
      " 46\n",
      " 43\n",
      " 43\n",
      " 43\n",
      " 27\n",
      " 27\n",
      " 27\n",
      " 27\n",
      " 27\n",
      " 27\n",
      " 27\n",
      " 27\n",
      " 62\n",
      " 62\n",
      " 62\n",
      " 62\n",
      "[torch.LongTensor of size 128]\n",
      ", 'crop_features': \n",
      " 5.2289e-02 -9.6440e-04 -1.2460e-01  ...  -7.9285e-03  1.9190e-01  1.2083e-02\n",
      " 5.2289e-02 -9.6440e-04 -1.2460e-01  ...  -7.9285e-03  1.9190e-01  1.2083e-02\n",
      "-1.4033e-01 -2.1030e-01 -7.0088e-02  ...   8.2357e-02  3.2865e-02  1.1312e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 5.2011e-02  8.9400e-02  8.7381e-02  ...  -1.9942e-01  2.1936e-01 -2.6817e-01\n",
      " 5.2011e-02  8.9400e-02  8.7381e-02  ...  -1.9942e-01  2.1936e-01 -2.6817e-01\n",
      " 5.2011e-02  8.9400e-02  8.7381e-02  ...  -1.9942e-01  2.1936e-01 -2.6817e-01\n",
      "[torch.FloatTensor of size 128x4096]\n",
      ", 'spaital': \n",
      "-0.3352 -0.9196  0.6111  ...  -0.7538  0.4732  0.1658\n",
      "-0.3352 -0.9196  0.6111  ...  -0.7538  0.4732  0.1658\n",
      " 0.0370  0.4801  0.5578  ...   0.5685  0.2604  0.0884\n",
      "          ...             ⋱             ...          \n",
      "-1.0000 -1.0000  0.8559  ...  -0.8086  0.9280  0.1914\n",
      "-1.0000 -1.0000  0.8559  ...  -0.8086  0.9280  0.1914\n",
      "-1.0000 -1.0000  0.8559  ...  -0.8086  0.9280  0.1914\n",
      "[torch.FloatTensor of size 128x8]\n",
      ", 'answer': \n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 128]\n",
      ", 'img_features': \n",
      " 1.5624e-01  1.5926e-01 -1.0526e-01  ...  -7.2057e-02  2.6717e-02 -7.0783e-02\n",
      " 1.5624e-01  1.5926e-01 -1.0526e-01  ...  -7.2057e-02  2.6717e-02 -7.0783e-02\n",
      " 5.7916e-02  1.6434e-01 -1.5779e-01  ...  -1.5041e-01  2.6621e-01 -5.9986e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 1.4268e-01  4.8699e-02 -2.4753e-01  ...  -1.2216e-01  1.5659e-01  1.3542e-01\n",
      " 1.4268e-01  4.8699e-02 -2.4753e-01  ...  -1.2216e-01  1.5659e-01  1.3542e-01\n",
      " 1.4268e-01  4.8699e-02 -2.4753e-01  ...  -1.2216e-01  1.5659e-01  1.3542e-01\n",
      "[torch.FloatTensor of size 128x4096]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(od, batch_size=128, shuffle=False, num_workers=1, pin_memory=False)\n",
    "start = time()\n",
    "for sample in dataloader:\n",
    "    print(time()-start)\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
