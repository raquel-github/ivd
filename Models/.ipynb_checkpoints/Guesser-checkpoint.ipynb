{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guesser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/GuesserModel.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = list()\n",
    "with open('../data/guesswhat.valid.new.jsonl') as f:\n",
    "    content = f.readlines()\n",
    "    data = [json.loads(x.strip()) for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, test = data[:1500], data[1500:1510]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Embeddings Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj_embed_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all object categories\n",
    "obj_cat = set()\n",
    "for d in data:\n",
    "    for obj in d['objects']:\n",
    "        obj_cat.add(obj['category_id'])\n",
    "\n",
    "# create embedding for each category\n",
    "if use_cuda:\n",
    "    obj_embends = nn.Embedding(len(obj_cat), obj_embed_dim).cuda()   \n",
    "else:\n",
    "    obj_embends = nn.Embedding(len(obj_cat), obj_embed_dim)\n",
    "\n",
    "catid2embedid = dict()\n",
    "for i, cat in enumerate(obj_cat):\n",
    "    catid2embedid[cat] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_obj_embeds(catid):\n",
    "    \"\"\" Returns the object embedding for a category \"\"\"\n",
    "    obj_embed = autograd.Variable(torch.LongTensor([catid2embedid[catid]]))\n",
    "    if use_cuda:\n",
    "        return obj_embends(obj_embed.cuda()).view(1,-1)\n",
    "    else:\n",
    "        return obj_embends(obj_embed).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spatial(bbox, image):\n",
    "    \"\"\" get normalized [-1, 1] spatial information of image \"\"\"\n",
    "    # get image coordinates\n",
    "    width = image['width']\n",
    "    height = image['height']\n",
    "    image_center_x = width / 2\n",
    "    image_center_y = height / 2\n",
    "\n",
    "    # clalc. norm. coords (between -1 and 1)\n",
    "    x_min = (min(bbox[0], bbox[2]) - image_center_x) / image_center_x\n",
    "    y_min = (min(bbox[1], bbox[3]) - image_center_y) / image_center_y\n",
    "    x_max = (max(bbox[0], bbox[2]) - image_center_x) / image_center_x\n",
    "    y_max = (max(bbox[1], bbox[3]) - image_center_y) / image_center_y\n",
    "    x_center = (x_min + x_max) / 2\n",
    "    y_center = (y_min + y_max) / 2\n",
    "    \n",
    "    w_box = x_max - x_min\n",
    "    h_box = y_max - y_min       \n",
    "    \n",
    "    if use_cuda:\n",
    "        return autograd.Variable(torch.FloatTensor([x_min, y_min, x_max, y_max, x_center, y_center, w_box, h_box])).cuda()\n",
    "    else:\n",
    "        return autograd.Variable(torch.FloatTensor([x_min, y_min, x_max, y_max, x_center, y_center, w_box, h_box]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Vocablurary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc_question(q):\n",
    "    q = q.lower()\n",
    "    q = '-SOS- ' + q[:q.index('?')] + str(' ? -EOS-')\n",
    "    return q\n",
    "\n",
    "def preproc_answer(a):\n",
    "    a = a.lower()\n",
    "    assert a in ['n/a', 'no', 'yes'], \"Unknown answer detected. Given %s, expected 'n/a', 'no' or 'yes'\" %answer\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocablurary created containing 1665 tokens.\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "\n",
    "# read the question and answer from the data to create vocablurary\n",
    "for d in data:\n",
    "    qas = d['qas']\n",
    "    for qna in qas:\n",
    "        question = preproc_question(qna['question'])\n",
    "        vocab.update(question.split())\n",
    "    \n",
    "        answer = preproc_answer(qna['answer'])\n",
    "        vocab.add(answer)\n",
    "    \n",
    "# create lookup dictionary \n",
    "word2index = dict()\n",
    "index2word = dict()\n",
    "for i, w in enumerate(vocab):\n",
    "    word2index[w] = i\n",
    "    index2word[i] = w\n",
    "\n",
    "print(\"Vocablurary created containing %i tokens.\" %(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM for Question History\n",
    "Takes all questions and answers as input.\n",
    "Last hidden state will be used for object guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Word Emebeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_word_embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors from ./glove.6B.100d.pt\n",
      "Loaded 400000 words\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import load_word_vectors\n",
    "\n",
    "wv_dict, wv_arr, wv_size = load_word_vectors('.', 'glove.6B', glove_word_embedding_size)\n",
    "\n",
    "print('Loaded', len(wv_arr), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add SOS and EOS tokens to dict\n",
    "if '-SOS-' not in wv_dict:\n",
    "    wv_dict['-SOS-'] = len(wv_dict)\n",
    "    temp = torch.cat([wv_arr, torch.randn((1,glove_word_embedding_size))])\n",
    "    wv_arr = temp\n",
    "if '-EOS-' not in wv_dict:\n",
    "    wv_dict['-EOS-'] = len(wv_dict)\n",
    "    temp = torch.cat([wv_arr, torch.randn((1,glove_word_embedding_size))])\n",
    "    wv_arr = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wv(w):\n",
    "    \"\"\" returning the word embedding \"\"\"\n",
    "    assert type(w) == str, \"Not given a string.\"\n",
    "    \n",
    "    if w in wv_dict:\n",
    "        vw = wv_arr[wv_dict[w]]\n",
    "        return vw.view(1, glove_word_embedding_size)\n",
    "    else:\n",
    "        print('Word not in Vocab: %s' %(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = glove_word_embedding_size\n",
    "hidden_dim = 100\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_in = 8 + obj_embed_dim # 8 comes from the spatial\n",
    "d_h1 = 100\n",
    "d_h2 = 50\n",
    "d_out = hidden_dim # where the output dimension must match the QGen hidden state dimension!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters MLP for Guesser vs. New Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GvQ_d_in = hidden_dim\n",
    "GvQ_d_h1 = 30\n",
    "GvQ_out = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Guesser(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, d_in, d_h1, d_h2, d_out):\n",
    "        super(Guesser, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        \n",
    "        # MLP model for making a decision whether to continue asking questions\n",
    "        # or making a guess\n",
    "        self.decision_model = nn.Sequential(\n",
    "            nn.Linear(GvQ_d_in, GvQ_d_h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(GvQ_d_h1, GvQ_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # MLP to provide object embedding based on spatial information and object category\n",
    "        # output will participate in object prediction\n",
    "        self.object_model = nn.Sequential(\n",
    "            nn.Linear(d_in, d_h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_h1, d_h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_h2, d_out)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "        if use_cuda:\n",
    "            return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)).cuda(),\n",
    "                    autograd.Variable(torch.zeros(1, 1, self.hidden_dim)).cuda())\n",
    "        else:\n",
    "            return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                    autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, dialogue, objects, image, hidden_QGen, training=True):\n",
    "        \n",
    "        # Word embedding of dialogue\n",
    "        if use_cuda:\n",
    "            embeds = autograd.Variable(torch.FloatTensor(len(dialogue.split()), embedding_dim)).cuda()\n",
    "        else:\n",
    "            embeds = autograd.Variable(torch.FloatTensor(len(dialogue.split()), embedding_dim))\n",
    "            \n",
    "        for i, w in enumerate(dialogue.split()):\n",
    "            embeds[i] = get_wv(w)\n",
    "        \n",
    "        \n",
    "        # lstm pass\n",
    "        out, self.hidden = self.lstm(embeds.view(len(dialogue.split()), 1, -1), self.hidden)\n",
    "        \n",
    "        \n",
    "        # make decision whether to continue asking questions or make a guess\n",
    "        if decision_model(hidden_QGen.view(-1,1)) > 0.5:\n",
    "            # Start Guessing\n",
    "        \n",
    "            # process objects\n",
    "            if use_cuda:\n",
    "                proposed_embeddings = autograd.Variable(torch.zeros(len(d['objects']), hidden_dim)).cuda()\n",
    "            else:\n",
    "                proposed_embeddings = autograd.Variable(torch.zeros(len(d['objects']), hidden_dim))\n",
    "\n",
    "            for i, obj in enumerate(objects):\n",
    "                x = get_spatial(obj['bbox'], image).view(1,-1)\n",
    "                e = get_obj_embeds(catid=obj['category_id'])\n",
    "                y = torch.cat([x, e], dim=1)\n",
    "                proposed_embeddings[i] = self.object_model(y)\n",
    "\n",
    "            if not training:\n",
    "                print(proposed_embeddings)\n",
    "                print(self.hidden[0].view(hidden_dim,-1))\n",
    "\n",
    "\n",
    "            # multiply lstm output with mlp output and apply softmax    \n",
    "            predicted_object = F.log_softmax(torch.mm(proposed_embeddings, self.hidden[0].view(hidden_dim,-1)).t())\n",
    "\n",
    "\n",
    "            return predicted_object\n",
    "        \n",
    "        else:\n",
    "            # Do not guess, but ask new question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Time 77.22 Loss 133.516113\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115182a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Guesser(embedding_dim=embedding_dim, hidden_dim=hidden_dim, vocab_size=vocab_size, \n",
    "                     d_in=d_in, d_h1=d_h1, d_h2=d_h2, d_out=d_out)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "avg_loss = list()\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    _loss = list()\n",
    "    start_time = time()\n",
    "    for d in data:\n",
    "        \n",
    "        # set gradient and hidden layer back to 0\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        \n",
    "        # get objects\n",
    "        object_ids = list()\n",
    "        objects = d['objects']\n",
    "        \n",
    "        for i, obj in enumerate(d['objects']):\n",
    "            object_ids.append(obj['id'])            \n",
    "        \n",
    "        # get image meta\n",
    "        image = d['image']\n",
    "        \n",
    "        # get the lstm input\n",
    "        dialogue = str()\n",
    "        for qna in d['qas']:\n",
    "            question = preproc_question(qna['question'])\n",
    "            answer = preproc_answer(qna['answer'])\n",
    "            dialogue += question + ' ' + answer + ' '\n",
    "          \n",
    "        \n",
    "        # check if embeddings for all words in the dialogue exist\n",
    "        wv_for_dialogue = True\n",
    "        for w in dialogue.split():\n",
    "            if w not in wv_dict:\n",
    "                wv_for_dialogue = False\n",
    "                break\n",
    "                \n",
    "        if wv_for_dialogue == False:\n",
    "            continue\n",
    "        \n",
    "        # make prediction\n",
    "        predicted_object = model(dialogue, objects, image)\n",
    "        \n",
    "        \n",
    "        # get target\n",
    "        target_id = d['object_id']\n",
    "        if use_cuda:\n",
    "            target = autograd.Variable(torch.LongTensor([object_ids.index(target_id)])).cuda()\n",
    "        else:\n",
    "            target = autograd.Variable(torch.LongTensor([object_ids.index(target_id)]))\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_function(predicted_object, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()           \n",
    "        \n",
    "        \n",
    "        # bookkeeping\n",
    "        if use_cuda:\n",
    "            loss = loss.cpu()\n",
    "        _loss.append(loss.data.numpy())\n",
    "        \n",
    "    avg_loss.append(numpy.mean(_loss))\n",
    "    print('Epoch %i Time %.2f Loss %f' %(epoch, time()-start_time, avg_loss[-1]))\n",
    "\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4\n",
      "[torch.LongTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = test[2]\n",
    "object_ids = list()\n",
    "objects = d['objects']\n",
    "\n",
    "for i, obj in enumerate(d['objects']):\n",
    "    object_ids.append(obj['id'])            \n",
    "\n",
    "\n",
    "# get the lstm input\n",
    "dialogue = str()\n",
    "for qna in d['qas']:\n",
    "    question = preproc_question(qna['question'])\n",
    "    answer = preproc_answer(qna['answer'])\n",
    "    dialogue += question + ' ' + answer + ' '\n",
    "    \n",
    "target_id = d['object_id']\n",
    "\n",
    "if use_cuda:\n",
    "    target = autograd.Variable(torch.LongTensor([object_ids.index(target_id)])).cuda()\n",
    "else:\n",
    "    target = autograd.Variable(torch.LongTensor([object_ids.index(target_id)]))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9cda3c22e31c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialogue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'image'"
     ]
    }
   ],
   "source": [
    "model(dialogue, objects, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
