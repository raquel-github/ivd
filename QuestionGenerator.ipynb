{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/QGenModel.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy\n",
    "from PIL import Image\n",
    "from VGG_Feature_Extract import VGG_Feature_Extract\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors from ./glove.6B.100d.pt\n",
      "Loaded 400000 words\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import load_word_vectors\n",
    "\n",
    "wv_dict, wv_arr, wv_size = load_word_vectors('.', 'glove.6B', 100)\n",
    "\n",
    "print('Loaded', len(wv_arr), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_dict['-SOS-'] = len(wv_dict)\n",
    "temp = torch.cat([wv_arr, torch.randn((1,100))])\n",
    "wv_arr = temp\n",
    "wv_dict['-EOS-'] = len(wv_dict)\n",
    "temp = torch.cat([wv_arr, torch.randn((1,100))])\n",
    "wv_arr = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wv(w):\n",
    "    \"\"\" returning the word embedding \"\"\"\n",
    "    assert type(w) == str, \"Not given a string.\"\n",
    "    \n",
    "    if w in wv_dict:\n",
    "        vw = wv_arr[wv_dict[w]]\n",
    "        return vw.view(1, 100)\n",
    "    else:\n",
    "        print('Word not in Vocab: %s' %(w))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 questions loaded.\n",
      "5 images will be loaded.\n"
     ]
    }
   ],
   "source": [
    "data = list()\n",
    "data.append('-SOS- is it a person ? -EOS-')\n",
    "data.append('-SOS- is it a dog ? -EOS-')\n",
    "data.append('-SOS- is it on the left side ? -EOS-')\n",
    "data.append('-SOS- is it on the ground ? -EOS-')\n",
    "data.append('-SOS- is it in the sky ? -EOS-')\n",
    "data.append('-SOS- is it an animal ? -EOS-')\n",
    "data.append('-SOS- is it an apple ? -EOS-')\n",
    "data.append('-SOS- is it red ? -EOS-')\n",
    "data.append('-SOS- is it a cat ? -EOS-')\n",
    "data.append('-SOS- is it on the right side ? -EOS-')\n",
    "data.append('-SOS- is it at the top ? -EOS-')\n",
    "data.append('-SOS- is it at the bottom ? -EOS-')\n",
    "data.append('-SOS- is it blue ? -EOS-')\n",
    "data.append('-SOS- are they in the foreground ? -EOS-')\n",
    "data.append('-SOS- are they in the background ? -EOS-')\n",
    "data.append('-SOS- are they wearing white ? -EOS-')\n",
    "data.append('-SOS- are they wearing pants ? -EOS-')\n",
    "data.append('-SOS- are they eating pizza ? -EOS-')\n",
    "data.append('-SOS- are they eating a sandwich ? -EOS-')\n",
    "data.append('-SOS- are they playing guitar ? -EOS-')\n",
    "\n",
    "print(\"%i questions loaded.\" %(len(data)))\n",
    "\n",
    "image_paths = list()\n",
    "image_paths.append('val2014/COCO_val2014_000000000042.jpg')\n",
    "image_paths.append('val2014/COCO_val2014_000000000073.jpg')\n",
    "image_paths.append('val2014/COCO_val2014_000000000074.jpg')\n",
    "image_paths.append('val2014/COCO_val2014_000000000133.jpg')\n",
    "image_paths.append('val2014/COCO_val2014_000000000136.jpg')\n",
    "\n",
    "print(\"%i images will be loaded.\" %(len(image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get image features\n",
    "VGG_ex = VGG_Feature_Extract(image_paths)\n",
    "image_features = list()\n",
    "for img_p in image_paths:\n",
    "    image_features.append(VGG_ex.get_features(img_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pants': 0, 'side': 1, 'a': 2, 'playing': 3, '-SOS-': 4, 'ground': 5, 'background': 6, 'pizza': 7, 'white': 8, 'sandwich': 9, 'they': 10, 'dog': 11, 'in': 12, 'red': 13, 'cat': 14, 'it': 15, 'on': 16, '-EOS-': 17, 'sky': 18, 'at': 19, 'are': 20, '?': 21, 'right': 22, 'animal': 23, 'apple': 24, 'is': 25, 'top': 26, 'left': 27, 'guitar': 28, 'wearing': 29, 'blue': 30, 'foreground': 31, 'eating': 32, 'the': 33, 'bottom': 34, 'an': 35, 'person': 36}\n"
     ]
    }
   ],
   "source": [
    "vocab = set(' '.join(data).split())\n",
    "word2index = dict()\n",
    "index2word = dict()\n",
    "for i, w in enumerate(vocab):\n",
    "    word2index[w] = i\n",
    "    index2word[i] = w\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "vocab_size = len(vocab)\n",
    "target_size = len(vocab)\n",
    "iterations = 10\n",
    "feature_dim = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QGen(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(QGen, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim+feature_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to word space\n",
    "        self.hidden2word = nn.Linear(hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if use_cuda:\n",
    "            return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)).cuda(),\n",
    "                    autograd.Variable(torch.zeros(1, 1, self.hidden_dim)).cuda())\n",
    "        else:\n",
    "            return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                    autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence, hidden=None):\n",
    "        \n",
    "        if hidden == None:\n",
    "            hidden = self.hidden\n",
    "            \n",
    "        # Word embedding of sentence\n",
    "        if use_cuda:\n",
    "            embeds = autograd.Variable(torch.FloatTensor(len(sentence.split()), embedding_dim)).cuda()\n",
    "        else:\n",
    "            embeds = autograd.Variable(torch.FloatTensor(len(sentence.split()), embedding_dim))\n",
    "\n",
    "        for i, w in enumerate(sentence.split()):\n",
    "            embeds[i] = get_wv(w)\n",
    "\n",
    "        # LSTM\n",
    "        if use_cuda:\n",
    "            visual_features = autograd.Variable(torch.randn(len(sentence.split()), 1, feature_dim)).cuda()\n",
    "        else:\n",
    "            visual_features = autograd.Variable(torch.randn(len(sentence.split()), 1, feature_dim))\n",
    "            \n",
    "\n",
    "        word_embeddings = embeds.view(len(sentence.split()), 1, -1)\n",
    "\n",
    "        lstm_in = torch.cat([word_embeddings, visual_features], dim=2)\n",
    "\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(lstm_in, hidden)\n",
    "        \n",
    "        \n",
    "        # mapping hidden state to word output\n",
    "        word_space = self.hidden2word(lstm_out.view(len(sentence.split()), -1))\n",
    "\n",
    "        \n",
    "        # p(w)\n",
    "        word_scores = F.log_softmax(word_space)\n",
    "        \n",
    "        return word_scores, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 3.461645\n",
      "Epoch 1 Loss 2.923511\n",
      "Epoch 2 Loss 2.713959\n",
      "Epoch 3 Loss 2.428927\n",
      "Epoch 4 Loss 2.371412\n",
      "Epoch 5 Loss 2.250130\n",
      "Epoch 6 Loss 2.270658\n",
      "Epoch 7 Loss 2.062148\n",
      "Epoch 8 Loss 1.990679\n",
      "Epoch 9 Loss 2.030633\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Init Model, loss function and optimizer\n",
    "model = QGen(embedding_dim=embedding_dim, hidden_dim=hidden_dim, vocab_size=vocab_size, target_size=target_size)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# See what the scores are before training\n",
    "inputs = ' '.join(data[0].split()[:-1])\n",
    "word_scores, _ = model(inputs)\n",
    "\n",
    "avg_loss = []\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    _loss = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        \n",
    "        # clear gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # clear hidden state for new datapoint\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # get the input to the LSTM\n",
    "        sentence_in = ' '.join(sentence.split()[:-1])\n",
    "\n",
    "        # get the desired output\n",
    "        if use_cuda:\n",
    "            targets = autograd.Variable(torch.LongTensor(len(sentence.split())-1)).cuda()\n",
    "        else:\n",
    "            targets = autograd.Variable(torch.LongTensor(len(sentence.split())-1))\n",
    "            \n",
    "        for i, w in enumerate(sentence.split()[1:]):\n",
    "            targets[i] = word2index[w]\n",
    "            \n",
    "            \n",
    "        # run our forward pass.\n",
    "        word_scores, _ = model(sentence_in)\n",
    "\n",
    "        # compute loss and do SGD\n",
    "        loss = loss_function(word_scores, targets)\n",
    "        if use_cuda:\n",
    "            loss = loss.cpu()\n",
    "        _loss.append(loss.data.numpy())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss.append(numpy.mean(_loss))\n",
    "    \n",
    "    \n",
    "    print('Epoch %i Loss %f' %(epoch, avg_loss[-1]))\n",
    "    \n",
    "print('Training completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8lOWd9/HPL+cjBEhACISEICiigAZEUEFaW2u7aqvb\nbVWqVIsobnW33bbbZ9tXq312u+3Wrc96xLMWa+ux1rZrbQVElENAQDkazichBBJIQsjp9/wxI0Ia\nyBAmuTOT7/v1yos5XMx8HeXrzXXdc93m7oiISHxJCDqAiIhEn8pdRCQOqdxFROKQyl1EJA6p3EVE\n4pDKXUQkDqncRUTikMpdRCQOqdxFROJQUlBvnJub64WFhUG9vYhITFq6dOled89ra1xg5V5YWEhp\naWlQby8iEpPMbEsk4zQtIyISh1TuIiJxSOUuIhKHVO4iInFI5S4iEodU7iIicUjlLiISh2Ku3Dft\nreHHv19FQ1Nz0FFERLqsGCz3ap5YsJlXl+8MOoqISJcVc+V+yfC+DO+XzcNvbaC5WRf3FhFpTcyV\nu5lx6+Ri1u+u5s21e4KOIyLSJcVcuQN84Zz+5Oek88DcMtx19C4i0lJMlntSYgLTLx7Csq2VLNm8\nP+g4IiJdTpvlbmZpZrbYzFaY2Soz+/EJxl5tZm5mJdGN+be+XDKI3pkpPDRvQ0e/lYhIzInkyP0w\nMMXdRwGjgcvMbHzLQWaWDdwBLIpuxNalpyQybUIhb67dw5pdBzrjLUVEYkab5e4h1eG7yeGf1ia6\n7wb+E6iLXrwTm3rBYDJSEnlYR+8iIseIaM7dzBLNbDmwB3jD3Re1eP5cYJC7/6EDMh5XTkYK144r\n4Pcrd7FtX21nvrWISJcWUbm7e5O7jwYGAuPMbOTHz5lZAnAP8K22XsfMpptZqZmVlpeXtzfzMW66\nqIgEg0fnb4zK64mIxIOTOlvG3SuBOcBlRz2cDYwE5prZZmA88Gpri6ruPsvdS9y9JC+vzUsARqR/\nz3SuGp3Pc0u2sbf6cFReU0Qk1kVytkyemeWEb6cDlwJrP37e3avcPdfdC929EFgIXOHunXaB1Fsm\nDaG+qZmn3tncWW8pItKlRXLk3h+YY2YrgSWE5txfM7O7zOyKjo0XmaF9s/nMiH48/e4Wqg83Bh1H\nRCRwSW0NcPeVwJhWHv/hccZPPvVYJ2/GpGJeX7Wb5xZv5eaLhgQRQUSky4jJb6i2ZkxBL8YP6c2j\n8zdR36jtgEWke4ubcge4dfJQPjpQxyvLdwQdRUQkUHFV7hefnsuI/j14aJ62AxaR7i2uyt3MmDG5\nmI3lNfx59e6g44iIBCauyh3g8pGnUdA7gwfnbdB2wCLSbcVduX+8HfCKbZUs3Lgv6DgiIoGIu3IH\nuOa8geRmpfKgNhQTkW4qLss9LTmRaRMLeWt9OR/sqAo6johIp4vLcge4fvxgslKTePgtbSgmIt1P\n3JZ7z/RkrhtfwB9W7mRLRU3QcUREOlXcljvATROLSEpIYJaO3kWkm4nrcu/bI42rz8vn+aXbKT+o\n7YBFpPuI63IHmH5xMQ1NzTyxYFPQUUREOk3cl3tRbiaXj+zPM+9u4UBdQ9BxREQ6RdyXO4S2Az54\nuJFnF20NOoqISKfoFuV+9sCeXDg0l8fe3kRdQ1PQcUREOly3KHeAWycXU37wMC+/p+2ARST+dZty\nn1Dch3MG9uTheRto0nbAIhLnIrlAdpqZLTazFWa2ysx+3MqYfzaz1Wa20sz+amaDOyZu+5kZMyYV\ns7miltdXfRR0HBGRDhXJkfthYIq7jwJGA5eZ2fgWY94DStz9HOAF4GfRjRkdnz3rNIpyM3lwrrYD\nFpH41ma5e0h1+G5y+MdbjJnj7rXhuwuBgVFNGSWJCcYtFw/h/R1VLCirCDqOiEiHiWjO3cwSzWw5\nsAd4w90XnWD4TcCfjvM6082s1MxKy8vLTz5tFHzx3Hz6Zqfy4LyyQN5fRKQzRFTu7t7k7qMJHZGP\nM7ORrY0zs+uBEuDnx3mdWe5e4u4leXl57c18SlKTErnpwiIWlFWwcntlIBlERDraSZ0t4+6VwBzg\nspbPmdmngf8DXOHuXXojl2vPLyA7LYmHdDEPEYlTkZwtk2dmOeHb6cClwNoWY8YADxMq9j0dETSa\nstOS+doFg/nTBx+xsby67d8gIhJjIjly7w/MMbOVwBJCc+6vmdldZnZFeMzPgSzgeTNbbmavdlDe\nqLlxQhHJiQk8Ml/bAYtI/Elqa4C7rwTGtPL4D4+6/eko5+pwedmpfLlkIL9dsp07Pz2Mfj3Sgo4k\nIhI13eYbqq2ZflExjc3NPP62tgMWkfjSrcu9oE8Gnz9nALMXbaXqkLYDFpH40a3LHWDGpCFUH27k\nVwu3BB1FRCRqun25nzWgJ5OG5fHEAm0HLCLxo9uXO4S2A95bXc/zS7cHHUVEJCpU7sD5Rb0ZPSiH\nR97aSGNTc9BxREROmcqd0HbAt04uZuu+Wv74gbYDFpHYp3IPu/TMfhTnaTtgEYkPKvewhATjlknF\nrNl1gLc+3Bt0HBGRU6JyP8pVo/M5rUcaD87VdsAiEttU7kdJSUrg5ouKWLhxH8u27g86johIu6nc\nW/jquAJ6pifz0FxtBywisUvl3kJmahI3XDCYP6/eTdmeg0HHERFpF5V7K26YUEhacgIPz9N2wCIS\nm1TureiTlcpXxhbwyvId7Ko6FHQcEZGTpnI/jpsuLKLZ4bH52g5YRGKPyv04BvXO4IpRA3h28VYq\na+uDjiMiclJU7idwy6Qh1NY38fS72g5YRGJLJBfITjOzxWa2wsxWmdmPWxmTama/MbMyM1tkZoUd\nEbaznXFaDz51Rl+efGczh+q1HbCIxI5IjtwPA1PcfRQwGrjMzMa3GHMTsN/dhwL/DfxndGMGZ8bk\nYvbV1PPb0m1BRxERiVib5e4h1eG7yeGfljtrXQk8Fb79AvApM7OopQzQ2MLelAzuxay3NtKg7YBF\nJEZENOduZolmthzYA7zh7otaDMkHtgG4eyNQBfRp5XWmm1mpmZWWl5efWvJOdOvkYnZUHuK1lTuD\njiIiEpGIyt3dm9x9NDAQGGdmI9vzZu4+y91L3L0kLy+vPS8RiEuG92V4v2wemrtR2wGLSEw4qbNl\n3L0SmANc1uKpHcAgADNLAnoCFdEI2BWEtgMewrrdB5mzbk/QcURE2hTJ2TJ5ZpYTvp0OXAqsbTHs\nVeCG8O1rgDc9zg5x/27UAPJz0nlQG4qJSAyI5Mi9PzDHzFYCSwjNub9mZneZ2RXhMY8BfcysDPhn\n4HsdEzc4yYkJfOOiIpZs3k/p5n1BxxEROSEL6gC7pKTES0tLA3nv9qqtb2TiT9/kvMG9ePSGsUHH\nEZFuyMyWuntJW+P0DdWTkJGSxI0TivjLmj2s+0jbAYtI16VyP0lfu2AwGSmJPDxPc+8i0nWp3E9S\nr8wUvjqugFdX7GT7/tqg44iItErl3g43XVgEwKPaDlhEuiiVezsMyEnnqjH5zF60hReXbg86jojI\n31C5t9MPPj+CsYW9+dbzK/jpn9bS1BxXp/WLSIxTubdTz4xknvr6OK47v4CH5m3glmeWUn24MehY\nIiKAyv2UJCcm8JOrRvLjK85izro9XPPgO1pkFZEuQeV+isyMGyYU8uS0seyoPMSV9y3QN1hFJHAq\n9yi56PQ8Xpk5kR7pyVz7yCJe0EKriARI5R5FxXlZvHzbBMYW9eLbz6/gP/60RgutIhIIlXuU5WSk\n8OS0cVw/voCH523klmdKtdAqIp1O5d4BQgutZ3PXlWcxZ1051zz4Dtv2aaFVRDqPyr0Dfe2C0ELr\nzspDXHX/ApZooVVEOonKvYMdu9C6kOdLtwUdSUS6AZV7JxiSl8Urt01kXFFv/uWFlfzHH7XQKiId\nS+XeSXpmJPPktHFMHT+Yh9/ayPSntdAqIh0nkmuoDjKzOWa22sxWmdkdrYzpaWa/N7MV4THTOiZu\nbEtOTODuq0Zy95VnMXd9OVc/oIVWEekYkRy5NwLfcvcRwHhgppmNaDFmJrDa3UcBk4FfmFlKVJPG\nkakXFPLUtHHsqjrElVpoFZEO0Ga5u/sud18Wvn0QWAPktxwGZJuZAVnAPkL/U5DjuPD0XF6ZOZEc\nLbSKSAc4qTl3MysExgCLWjx1H3AmsBN4H7jD3ZujkC+uDcnL4uXbJnJ+UR/+5YWV/LsWWkUkSiIu\ndzPLAl4E7nT3Ay2e/iywHBgAjAbuM7MerbzGdDMrNbPS8vLyU4gdP0ILrWO54YLBzAovtB6sawg6\nlojEuIjK3cySCRX7bHd/qZUh04CXPKQM2ASc0XKQu89y9xJ3L8nLyzuV3HElKTGBH185kruvGhla\naNU3WkXkFEVytowBjwFr3P2e4wzbCnwqPL4fMBzYGK2Q3cXU8YN5+uvj2H3gMFfev4DFm7TQKiLt\nE8mR+0RgKjDFzJaHfy43sxlmNiM85m5ggpm9D/wV+K677+2gzHFt4tDwQmtGMtc9upDfLtFCq4ic\nPHMPZgGvpKTES0tLA3nvWFB1qIHbn13G/A/38o2Livje584kMcGCjiUiATOzpe5e0tY4fUO1i+qZ\nnswTN4YWWh+Zv4lvaKFVRE6Cyr0L+3ih9SdXjWReeKF1a4UWWkWkbSr3GHD9+ME8c2Sh9W0WbawI\nOpKIdHEq9xgxIbzQ2iszhesfW8RvlmwNOpKIdGEq9xhSlJvJy7dNZPyQPnz3xff5yWur9Y1WEWmV\nyj3GfLzQeuOEQh59exM3P7VEC60i8jdU7jEoKTGBH11xFv/3iyOZ/+FevjJroQpeRI6hco9h150/\nmEe+VsLajw5y2+xlNDRprzYRCVG5x7hLzujLf3zpbOZ/uJfvvfg+QX0pTUS6lqSgA8ip+3LJIHZV\n1vHff1lPfk4a//yZ4UFHEpGAqdzjxDc/NZSdlYf4f2+W0T8nna+OKwg6kogESOUeJ8yMn3xxJLsP\n1vFvr3xAvx6pTDmjX9CxRCQgmnOPI8mJCdx/7bmc2T+bmbPfY+X2yqAjiUhAVO5xJjM1icdvHEuf\nrBS+/uQS7UUj0k2p3ONQ3+w0nvr6OBqbnRueWMy+mvqgI4lIJ1O5x6nivCwe/VoJOyoPcfNTS6hr\naAo6koh0IpV7HCsp7M29/zCa97ZVcsdz72kfGpFuROUe5z53dn9++IURvL5qN3f9fpW+5CTSTURy\ngexBZjbHzFab2Sozu+M44yaHr6+6yszmRT+qtNe0iUV846Iinnp3C4/M13XLRbqDSM5zbwS+5e7L\nzCwbWGpmb7j76o8HmFkO8ABwmbtvNbO+HZRX2ulfP3cmO6vq+Pc/ruW0nulcMWpA0JFEpAO1We7u\nvgvYFb590MzWAPnA6qOGXQu85O5bw+P2dEBWOQUJCcYv/n4U5QcP8+3friAvK5ULivsEHUtEOshJ\nzbmbWSEwBljU4qlhQC8zm2tmS83sa9GJJ9GUlpzII1NLKOiTwfRnSlm/+2DQkUSkg0Rc7maWBbwI\n3OnuB1o8nQScB3we+CzwAzMb1sprTDezUjMrLS8vP4XY0l49M5J5ctpY0pMTufHxxXxUVRd0JBHp\nABGVu5klEyr22e7+UitDtgOvu3uNu+8F3gJGtRzk7rPcvcTdS/Ly8k4lt5yCgb0yeGLaWKoONXDj\nE4t1oQ+ROBTJ2TIGPAascfd7jjPsd8CFZpZkZhnA+cCa6MWUaDtrQE8evP48yvZUc+uvllHfqAt9\niMSTSI7cJwJTgSnhUx2Xm9nlZjbDzGYAuPsa4H+BlcBi4FF3/6DDUktUXDwsj59efQ5vl+3ley+u\n1DnwInEkkrNl3gYsgnE/B34ejVDSea45byC7Kg/xizfWMyAnnW9/Vhf6EIkH2s9duH3KUHZWHeK+\nOWX0z0njuvMHBx1JRE6Ryl0wM+6+ciQfVdXxg1c+4LQeaXzqTF3oQySWaW8ZASApMYH7rj2Xkfk9\nuf3Z91i+TRf6EIllKnc5IjM1icduGEtudgo3PbmELRU1QUcSkXZSucsx8rJTeWraOJrdueHxxVRU\nHw46koi0g8pd/saQvCwevaGEXVV13Px0KYfqdaEPkVijcpdWnTe4N/d+ZQzLt1XyTV3oQyTmqNzl\nuC4beRo/+ruzeGP1bn70qi70IRJLdCqknNANEwrZWXmIh9/aSH6vdGZMKg46kohEQOUubfruZWew\ns6qOn/5pLf17pnHl6PygI4lIG1Tu0qaEBOO//v4c9hyo49vPryAvO5UJxblBxxKRE9Ccu0QkNSmR\nWV8roSg3k1ueXsraj1pu6S8iXYnKXSLWMz2ZJ6aNIyM1kWlPLGFX1aGgI4nIcajc5aTk56TzxI3j\nOFjXyLQnlnBAF/oQ6ZJU7nLSRgzowUPhC33MeGapLvQh0gWp3KVdLjw9l59dcw7vbKjgOy+s0Dnw\nIl2MzpaRdvvSuQPZVVXHz19fx4CcdL5z2RlBRxKRMJW7nJLbJhezo/IQD8zdwK6qOiYU9+G8wb0o\nys0kdPldEQlCm+VuZoOAp4F+gAOz3P3e44wdC7wLfMXdX4hmUOmazIy7rjgLgNdW7OTl93YA0Dsz\nhXMLelFS2IuSwb0Ymd+TtOTEIKOKdCvW1lypmfUH+rv7MjPLBpYCV7n76hbjEoE3gDrg8bbKvaSk\nxEtLS08pvHQtzc3OhvJqlm7ZT+mW/Szbsp+Ne0N7wqckJjAyvwfnDe4V/ulNXnZqwIlFYo+ZLXX3\nkjbHnexCmJn9DrjP3d9o8fidQAMwFnhN5S4AFdWHWba1ktIt+1i2ZT8rtlcdObumoHcGJYN7ce7g\n0BH+6X2zSUzQVI7IiURa7ic1525mhcAYYFGLx/OBLwKXECp3EQD6ZKVy6Yh+XDoidE3Ww41NrNp5\ngKWb97N0y37e+nAvL4WncrJTkxgzuBfnhadzRg3KIStVy0Ii7RHxnxwzywJeBO5095bfPf8l8F13\nbz7RIpqZTQemAxQUFJx8Wol5qUmJnFvQi3MLevENwN3Zuq/2mKmcX/51Pe6QYHBm/6OncnqRn5Ou\nhVqRCEQ0LWNmycBrwOvufk8rz28CPv4TlwvUAtPd/ZXjvaamZeR4qg41sHxbJUs372Pp1v28t7WS\n2vDVoE7rkXZM2Y8Y0IPkRH1dQ7qPqM25W+gw6Slgn7vfGcEbP4nm3CWKGpuaWfvRQZZu2X/kZ0dl\naF+btOQERg3M4bzwvP25Bb3IyUgJOLFIx4lmuV8IzAfeBz7+nvn3gQIAd3+oxfgnUblLB9tVdeiY\nsl+188CRSwF++sx+3D5lKKMH5QScUiT6OuxsmWhRuUs01dY3smJbFW+XlTN70VYqaxu46PRcZl4y\nlPOLemueXuKGyl26rerDjcxeuIVH5m9ib/VhSgb34vYpQ5k0LE8lLzFP5S7dXl1DE79Zso2H521g\nZ1UdZ+f3ZOYlQ/nMiH4k6Hx6iVEqd5Gw+sZmXn5vOw/O3cDmilqG9cvitslD+cI5/UnSmTYSY1Tu\nIi00NjXzh/d3cf+cMtbvrmZwnwxunVTMl84dSEqSSl5ig8pd5Diam5031uzm/jllrNxeRf+eadxy\n8RC+Mq5Am5tJl6dyF2mDu/PWh3u5/80yFm/eR25WCjdfNITrxw/WtgfSZancRU7Coo0V3DenjPkf\n7qVnejI3Tihk2sRCfSFKuhyVu0g7rNhWyX1zynhj9W4yUxK5/oLB3HzhEG1PLF2Gyl3kFKz96AD3\nz9nAH1buJDkxga+OK2D6xUMYkJMedDTp5lTuIlGwaW8ND84t46VlOzCDL40ZyK2TiynMzQw6mnRT\nKneRKNq+v5ZZb23kuSXbaGxq5u9GDWDmJUMZ1i876GjSzajcRTrAngN1PPr2Jn61cAu19U189qx+\n3H7J6Zw9sGeguZqbnQN1DVTU1LOvpp6K6tCv6SkJXDEqX1e4iiMqd5EOtL+mnife2cyTCzZxoK6R\ni4flcfslQxlX1Dsqr9/Y1My+2lBB76uup6Kmnv21n5T2vpp6KmoOH7m9v7bhyK6YLU05oy/3fmU0\n2WnJUckmwVK5i3SCg3UNPLNwC4/N30RFTT3jinpz+yVDuej03GM2KatraDqqlOvZV3P4mKI++qei\npp6qQw3Hfc+cjGR6Z6bQJzOF3pkp9M5MpXdmMr0zU496LIU+WSn8ZfVufvT71QzJzeSxG8ZS0Cej\nMz4W6UAqd5FOdKi+iV8v3sqstzby0YE6hvfLJi05IXTEXVNPTfhKUi0lJhi9Mo4q6qxPbvc5Utyf\nFHavjOST3g/nnbK93Dp7GQkGD1x3HhcU94nGP7IEROUuEoDDjU28tGwHLy/bQWpywpGC7pN11BH1\nkV9TyU5L6pQdKjfvreGmp5awpaKWu64cybXn6xrGsUrlLiLHOFDXwD8++x7z1pdz44RC/u3zZ2pX\nzBgUabnr36xIN9EjLZnHbxzLzRcW8eQ7m5n25BKqao8/ty+xrc1yN7NBZjbHzFab2Sozu6OVMdeZ\n2Uoze9/M3jGzUR0TV0RORWKC8W9fGMHPrj6HhRsr+OIDC9hYXh10LOkAkRy5NwLfcvcRwHhgppmN\naDFmEzDJ3c8G7gZmRTemiETTl8cO4tlvjKfyUANX3b+A+R+WBx1JoqzNcnf3Xe6+LHz7ILAGyG8x\n5h133x++uxAYGO2gIhJdYwt787uZExmQk86NTyzhyQWbCGoNTqLvpObczawQGAMsOsGwm4A/tT+S\niHSWQb0zeOHWCVwyvC8/+v1qvv/yB9Q3NgcdS6Ig4nI3syzgReBOdz9wnDGXECr37x7n+elmVmpm\npeXl+mugSFeQlZrErKnncdvkYn69eCtTH1vEvpr6oGPJKYqo3M0smVCxz3b3l44z5hzgUeBKd69o\nbYy7z3L3EncvycvLa29mEYmyhATjO5edwS//YTTvbavkyvvfZv3ug0HHklMQydkyBjwGrHH3e44z\npgB4CZjq7uujG1FEOstVY/L5zfTx1DU086UH3uGva3YHHUnaKZIj94nAVGCKmS0P/1xuZjPMbEZ4\nzA+BPsAD4ef17SSRGDWmoBev3j6RwtwMbn66lIfnbdBCawzSN1RFpFWH6pv49vMr+MP7u7j63IH8\n+5dGkpqUGHSsbk/fUBWRU5Keksh9147hnz49jBeXbeersxZSfvBw0LEkQip3ETkuM+OOT5/OA9ed\ny+pdB7jyvrdZtbMq6FgSAZW7iLTp8rP788KMCThwzYPv8r8f7Ao6krRB5S4iERmZ35PfzZzI8NOy\nmfGrZfzPXz/UQmsXpnIXkYj17ZHGc9PH88Ux+fzijfV887nl1DW0fiESCVZS0AFEJLakJSdyz5dH\nMaxfNj97fS1bKmqYNbWE03qmBR1NjqIjdxE5aWbGrZOLmTW1hA17qrnivrdZsa0y6FhyFJW7iLTb\npSP68eJtE0hJSuDLD7/L75bvCDqShKncReSUnHFaD343cyLnDOzJHc8t5xd/XkdzsxZag6ZyF5FT\n1icrldk3j+fLJQP5nzfLuHX2UmoONwYdq1tTuYtIVKQkJfCfV5/DD74wgjdW7+aah95l+/7aoGN1\nWyp3EYkaM+OmC4t4/MaxbN9Xy1X3L2Dpln1Bx+qWVO4iEnWTh/fl5ZkTyEpN4quzFvHo/I3al6aT\naVdIEekwlbX1zHx2GQvKQtfvGZnfg0nD8pg8vC9jBuWQlKjjy5MV6a6QKncR6VDuzqqdB5i3vpy5\n6/awbGslTc1OdloSF52ey+RhfZk0PI9+PfQlqEio3EWkS6o61MCCsr3MXbeHeevL2X0gNF1zxmnZ\nTB7el8nD8zhvcC+SdVTfKpW7iHR57s7ajw4yd13oqH7plv00NjtZqUlMHNqHycP7MmlYHgNy0oOO\nGhWHG5vYvLeWjJREBvXOaNdrqNxFJOYcrGtgQVkF89bvYe66cnZV1QEwrF9W6Kh+WB4lhb1JSera\nR/VVhxrYUF5N2Z5qNpRXs2FP6PbWfbU0O9wyaQj/+rkz2/XaUSt3MxsEPA30AxyY5e73thhjwL3A\n5UAtcKO7LzvR66rcReRE3J0P91Qzd12o6Jds3kdDk5ORksiE4lwmD89j8vA8BvZq3xFwNPJ9dKCO\nDXtqKNtzkLLy6tDt8upjzgxKTjSKcjMZ2jeL4rwshvbNYtTAHApzM9v1vpGWeyS7QjYC33L3ZWaW\nDSw1szfcffVRYz4HnB7+OR94MPyriEi7mBnD+mUzrF820y8upuZwI+9sqDhS9n9ZsxuA4rzMI3P1\nYwt7k5Yc3eu8NjQ1s6Wi9tij8PCvNfWfbHecnZpEcd8sJg3LO1LiQ/tmMahXeiBnBZ30tIyZ/Q64\nz93fOOqxh4G57v7r8P11wGR3P+7lWnTkLiLt5e5sKK85sii7aNM+6hubSU9O5ILiPqGj+mF9KegT\n+VF9zeHGY6ZSysJTKVsqamk8aq+c03qkhY/Cjz0az8tOJTSJ0bGieeR+9IsWAmOARS2eyge2HXV/\ne/gxXYtLRKLOzI4cGd980RBq6xtZuLGCeevKmbu+nDfX7gFWUZSbGT6vPo/xQ/qQmpRAefXhI9Mn\nG44q8o/n9wESE4zBfTIYmpfFZ846jaHhAh+Sl0l2WnJw/+AnIeJyN7Ms4EXgTnc/0J43M7PpwHSA\ngoKC9ryEiMjfyEhJYsoZ/ZhyRj8ANu395Kj+14u38uQ7m0lNSiAtOZGqQw1H/b5EivOyGD+kzzFH\n4wW9M7v8om1bIip3M0smVOyz3f2lVobsAAYddX9g+LFjuPssYBaEpmVOOq2ISASKcjMpyi1i2sQi\n6hqaWLixgrfW7+VwY9MxUyn9e6Z1ylRKENos9/CZMI8Ba9z9nuMMexW43cyeI7SQWnWi+XYRkc6S\nlpwYXnDtG3SUThXJkftEYCrwvpktDz/2faAAwN0fAv5I6DTIMkKnQk6LflQREYlUm+Xu7m8DJ/x7\ni4dOuZkZrVAiInJqYnvFQEREWqVyFxGJQyp3EZE4pHIXEYlDKncRkTikchcRiUOB7eduZuXAlnb+\n9lxgbxTjxDp9HsfS5/EJfRbHiofPY7C757U1KLByPxVmVhrJrmjdhT6PY+nz+IQ+i2N1p89D0zIi\nInFI5S5CHxdiAAACpUlEQVQiEoditdxnBR2gi9HncSx9Hp/QZ3GsbvN5xOScu4iInFisHrmLiMgJ\nxFy5m9llZrbOzMrM7HtB5wmSmQ0yszlmttrMVpnZHUFnCpqZJZrZe2b2WtBZgmZmOWb2gpmtNbM1\nZnZB0JmCYmb/FP4z8oGZ/drM0oLO1NFiqtzNLBG4H/gcMAL4qpmNCDZVoBqBb7n7CGA8MLObfx4A\ndwBrgg7RRdwL/K+7nwGMopt+LmaWD3wTKHH3kUAi8JVgU3W8mCp3YBxQ5u4b3b0eeA64MuBMgXH3\nXe6+LHz7IKE/vPnBpgqOmQ0EPg88GnSWoJlZT+BiQldRw93r3b0y2FSBSgLSzSwJyAB2Bpynw8Va\nuecD2466v51uXGZHM7NCYAywKNgkgfol8B2gOeggXUARUA48EZ6metTMMoMOFQR33wH8F7AV2EXo\nMqB/DjZVx4u1cpdWmFkWoQuY3+nuB4LOEwQz+wKwx92XBp2li0gCzgUedPcxQA3QLdeozKwXob/h\nFwEDgEwzuz7YVB0v1sp9BzDoqPsDw491W2aWTKjYZ7v7S0HnCdBE4Aoz20xoum6Kmf0q2EiB2g5s\nd/eP/yb3AqGy744+DWxy93J3bwBeAiYEnKnDxVq5LwFON7MiM0shtCjyasCZAmNmRmhOdY273xN0\nniC5+7+6+0B3LyT038Wb7h73R2fH4+4fAdvMbHj4oU8BqwOMFKStwHgzywj/mfkU3WBxuc0LZHcl\n7t5oZrcDrxNa8X7c3VcFHCtIE4GpwPtmtjz82Pfd/Y8BZpKu4x+B2eEDoY3AtIDzBMLdF5nZC8Ay\nQmeYvUc3+KaqvqEqIhKHYm1aRkREIqByFxGJQyp3EZE4pHIXEYlDKncRkTikchcRiUMqdxGROKRy\nFxGJQ/8fu8O2wqwZrccAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1f51208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(avg_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 100], m2: [4196 x 400] at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1237",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9262ead0a22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0minput_wv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_wv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_wv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mword_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mword_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mdropout_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         )\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# cuBLAS doesn't support 0 strides in sger, so we can't use expand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 100], m2: [4196 x 400] at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1237"
     ]
    }
   ],
   "source": [
    "# given the start of sentence token\n",
    "# we use beam search to approximate the most likely question\n",
    "\n",
    "inputs = '-SOS-'\n",
    "h_init = model.init_hidden() # clear the first hidden state\n",
    "max_lenght = 10 # max. question length\n",
    "top_k = 3 # take the top k succesor word of the SOS token\n",
    "p = [0]*3 # probability to track the sequence probability\n",
    "seq = [inputs, inputs, inputs]\n",
    "\n",
    "\n",
    "# get the first p(w) distribution\n",
    "# do first forward pass \n",
    "if use_cuda:\n",
    "    input_wv = autograd.Variable(get_wv(inputs)).cuda()\n",
    "else:\n",
    "    input_wv = autograd.Variable(get_wv(inputs))\n",
    "    \n",
    "out, h_first = model.lstm(input_wv.view(1,1,-1), h_init)\n",
    "word_space = model.hidden2word(out.view(1, -1))\n",
    "word_score = F.log_softmax(word_space)\n",
    "\n",
    "if use_cuda:\n",
    "    word_score = word_score.cpu()\n",
    "    \n",
    "idxs = word_score.data.numpy().ravel().argsort()[-top_k:][::-1]\n",
    "\n",
    "# get the top_k words and add them to the sequence\n",
    "p = word_score.data.numpy().ravel()[idxs]\n",
    "for i, idx in enumerate(idxs):\n",
    "    seq[i]+= ' ' + index2word[idx]\n",
    "\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    # get the first word and its embedding, set the hidden state\n",
    "    inputs = index2word[idx]\n",
    "    if use_cuda:\n",
    "        input_wv = autograd.Variable(get_wv(inputs)).cuda()\n",
    "    else:\n",
    "        input_wv = autograd.Variable(get_wv(inputs))\n",
    "        \n",
    "    h = h_first\n",
    "    \n",
    "    # keep forward passing and keep track of sequence and respective probability\n",
    "    for _ in range(1, max_lenght):\n",
    "        out, h = model.lstm(input_wv.view(1,1,-1), h)\n",
    "        word_space = model.hidden2word(out.view(1, -1))\n",
    "        word_score = F.log_softmax(word_space)\n",
    "        if use_cuda:\n",
    "            word_score = word_score.cpu()\n",
    "        p[i] += numpy.max(word_score.data.numpy().ravel())\n",
    "        inputs = index2word[numpy.argmax(word_score.data.numpy().ravel())]\n",
    "        seq[i] += ' ' + inputs\n",
    "        if use_cuda:\n",
    "            input_wv = autograd.Variable(get_wv(inputs)).cuda()\n",
    "        else:\n",
    "            input_wv = autograd.Variable(get_wv(inputs))\n",
    "        \n",
    "        # if -EOS- was emitted, stop\n",
    "        if inputs == '-EOS-':\n",
    "            break\n",
    "        \n",
    "print(p)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
